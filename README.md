# ðŸ§  Gradient Descent from Scratch â€“ A Hands-On Journey

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/amitabh1609/Gradient_Descent/blob/main/gradient_descent.ipynb)

This notebook is a complete walkthrough of how Gradient Descent works â€” from the basics to intuitive visualizations.

It started as a learning project and turned into something Iâ€™m proud to showcase for TA/RA opportunities. Iâ€™ve implemented the algorithm in multiple ways to understand not just how it works, but **why** it works the way it does.

---

## ðŸš€ What Youâ€™ll Find Inside

### ðŸ”¹ Version 1 â€“ Manual, Loop-Based Gradient Descent
- A straightforward, raw implementation that loops through each data point
- Great for building intuition about gradients and loss functions

### ðŸ”¹ Version 2 â€“ Vectorized Gradient Descent
- Cleaner and faster implementation using NumPy
- Includes a live loss curve to track learning progress

### ðŸ”¹ Version 3 â€“ 3D Gradient Descent Visualization
- Visualizes how gradient descent works over a 3D surface:  
  `z = sin(5x) * cos(5y)/5`
- Youâ€™ll see the optimization path in real time

---

## ðŸ”— Want to Try It?

Just click the badge at the top to run the notebook directly in Google Colab â€” no setup needed.

---

## ðŸ’¡ Why I Made This

I wanted to build more than just working code â€” I wanted to create something **explorable** and **teachable**.  
This notebook helped me reinforce:
- The math behind gradients
- Why vectorization matters in real-world ML
- How visualizations can make abstract concepts click

---

## ðŸ›  Built With

- Python & NumPy  
- Matplotlib for visualization  
- A lot of curiosity and debugging

---

## ðŸ§  Whatâ€™s Next?

- Extend this to multivariable regression  
- Add stochastic and mini-batch variants  
- Maybe even animate gradient flow using `matplotlib.animation`

---

Thanks for checking this out! Feel free to fork, run, or reach out if youâ€™re working on something similar ðŸ™Œ
